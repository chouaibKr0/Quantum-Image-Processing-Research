% chapters/chapter1.tex
% Chapter 1: The Classical Paradigm

\chapter{The Classical Paradigm: Image Segmentation via Clustering}

\section{Fundamentals and Core Definitions: Defining Clustering and the Segmentation Problem}

Before delving into quantum approaches, it is essential to establish a rigorous understanding of the classical framework for image segmentation through clustering. This section defines the fundamental concepts and mathematical formulations that underpin both classical and quantum methods.

\subsection{What is Clustering?}

Clustering is an unsupervised machine learning technique that aims to partition a dataset into groups (clusters) such that objects within the same cluster are more similar to each other than to objects in different clusters.

\begin{definitionbox}[Clustering]
Given a dataset $X = \{x_1, x_2, \ldots, x_n\}$ where $x_i \in \mathbb{R}^d$, clustering seeks to find a partition $\mathcal{C} = \{C_1, C_2, \ldots, C_k\}$ such that:
\begin{enumerate}
    \item $\bigcup_{i=1}^{k} C_i = X$ (completeness)
    \item $C_i \cap C_j = \emptyset$ for $i \neq j$ (mutual exclusivity in hard clustering)
    \item $C_i \neq \emptyset$ for all $i$ (non-emptiness)
\end{enumerate}
\end{definitionbox}

The quality of clustering is typically measured by optimizing an objective function. For example, the within-cluster sum of squares (WCSS) for K-means clustering:

\begin{equation}
    J = \sum_{i=1}^{k} \sum_{x \in C_i} \|x - \mu_i\|^2
\end{equation}

where $\mu_i$ is the centroid of cluster $C_i$.

\subsection{The Image Segmentation Problem}

Image segmentation is the process of partitioning a digital image into multiple segments, where each segment consists of pixels that share certain characteristics.

\begin{definitionbox}[Image Segmentation]
For an image $I$ defined over a domain $\Omega$, segmentation produces a partition $\{R_1, R_2, \ldots, R_n\}$ such that:
\begin{enumerate}
    \item $\bigcup_{i=1}^{n} R_i = \Omega$
    \item $R_i$ is connected for all $i$
    \item Pixels within each $R_i$ satisfy a homogeneity predicate $P(R_i) = \text{TRUE}$
    \item $P(R_i \cup R_j) = \text{FALSE}$ for adjacent regions $R_i$ and $R_j$
\end{enumerate}
\end{definitionbox}

In practice, pixels are represented as feature vectors that may include:

\begin{itemize}
    \item \textbf{Color features}: RGB, HSV, or Lab color space values
    \item \textbf{Spatial features}: Pixel coordinates $(x, y)$
    \item \textbf{Texture features}: Local binary patterns, Gabor filter responses
    \item \textbf{Gradient features}: Edge magnitude and orientation
\end{itemize}

\subsection{Clustering as a Segmentation Approach}

When clustering is applied to image segmentation, each pixel (or region) is treated as a data point in a feature space. The clustering algorithm groups similar pixels together, effectively segmenting the image.

\begin{notebox}
The connection between clustering and segmentation is natural: both seek to identify groups of similar elements. However, image segmentation introduces additional constraints, such as spatial coherence and the preservation of object boundaries.
\end{notebox}

The general pipeline for clustering-based segmentation involves:

\begin{enumerate}
    \item \textbf{Feature extraction}: Transform each pixel into a feature vector
    \item \textbf{Clustering}: Apply a clustering algorithm to group feature vectors
    \item \textbf{Label assignment}: Assign each pixel the label of its cluster
    \item \textbf{Post-processing}: Refine segment boundaries and remove noise
\end{enumerate}

\section{Classical Clustering Architectures: From K-Means to Spectral Graph Theory}

This section reviews the major classical clustering algorithms used for image segmentation, analyzing their mathematical foundations, strengths, and limitations.

\subsection{K-Means Clustering}

K-means is one of the most widely used clustering algorithms due to its simplicity and efficiency. It partitions data into $k$ clusters by iteratively refining cluster centroids.

\begin{algorithm}
\caption{K-Means Clustering}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Dataset $X = \{x_1, \ldots, x_n\}$, number of clusters $k$
\STATE \textbf{Output:} Cluster assignments and centroids
\STATE Initialize $k$ cluster centroids $\{\mu_1, \ldots, \mu_k\}$ randomly
\REPEAT
    \STATE \textbf{Assignment step:} Assign each $x_i$ to nearest centroid:
    \STATE \quad $c_i = \arg\min_j \|x_i - \mu_j\|^2$
    \STATE \textbf{Update step:} Recompute centroids:
    \STATE \quad $\mu_j = \frac{1}{|C_j|} \sum_{x_i \in C_j} x_i$
\UNTIL{convergence (centroids no longer change)}
\end{algorithmic}
\end{algorithm}

\textbf{Complexity Analysis:} Each iteration requires $O(nkd)$ operations, where $n$ is the number of data points, $k$ is the number of clusters, and $d$ is the dimensionality. For images, $n$ can be millions of pixels.

\begin{warningbox}
K-means has several limitations for image segmentation:
\begin{itemize}
    \item Assumes spherical (convex), equally-sized clusters
    \item Sensitive to initialization and outliers
    \item Requires specifying $k$ in advance
    \item Cannot capture complex cluster shapes
\end{itemize}
\end{warningbox}

\subsection{Hierarchical Clustering}

Hierarchical clustering builds a tree-like structure (dendrogram) of nested clusters without requiring the number of clusters to be specified beforehand.

\textbf{Agglomerative (bottom-up) approach:}
\begin{enumerate}
    \item Start with each point as its own cluster
    \item Iteratively merge the two closest clusters
    \item Continue until all points belong to a single cluster
\end{enumerate}

The distance between clusters can be computed using various linkage criteria:

\begin{itemize}
    \item \textbf{Single linkage:} $d(C_i, C_j) = \min_{x \in C_i, y \in C_j} d(x, y)$
    \item \textbf{Complete linkage:} $d(C_i, C_j) = \max_{x \in C_i, y \in C_j} d(x, y)$
    \item \textbf{Average linkage:} $d(C_i, C_j) = \frac{1}{|C_i||C_j|} \sum_{x \in C_i} \sum_{y \in C_j} d(x, y)$
    \item \textbf{Ward's method:} Minimizes within-cluster variance
\end{itemize}

\textbf{Complexity:} Standard hierarchical clustering has $O(n^3)$ time complexity and $O(n^2)$ space complexity, making it impractical for large images.

\subsection{Fuzzy C-Means}

Unlike hard clustering methods, Fuzzy C-Means (FCM) allows each data point to belong to multiple clusters with varying degrees of membership.

\begin{definitionbox}[Fuzzy C-Means Objective]
FCM minimizes the following objective function:
\begin{equation}
    J_m = \sum_{i=1}^{n} \sum_{j=1}^{c} u_{ij}^m \|x_i - v_j\|^2
\end{equation}
where $u_{ij}$ is the membership degree of $x_i$ in cluster $j$, $v_j$ is the cluster center, and $m > 1$ is the fuzziness parameter.
\end{definitionbox}

The membership values and cluster centers are updated iteratively:

\begin{equation}
    u_{ij} = \frac{1}{\sum_{k=1}^{c} \left(\frac{\|x_i - v_j\|}{\|x_i - v_k\|}\right)^{\frac{2}{m-1}}}
\end{equation}

\begin{equation}
    v_j = \frac{\sum_{i=1}^{n} u_{ij}^m x_i}{\sum_{i=1}^{n} u_{ij}^m}
\end{equation}

\begin{tipbox}
FCM is particularly useful for image segmentation when boundaries between regions are gradual or ambiguous, such as in medical imaging where tissue boundaries may be unclear.
\end{tipbox}

\subsection{Spectral Clustering and Graph Theory}

Spectral clustering uses eigenvalues of similarity matrices to perform dimensionality reduction before clustering. It can identify clusters with complex, non-convex shapes.

Unlike traditional clustering methods that rely on direct distance measures, spectral clustering captures the global structure of the image by analyzing how pixels are connected in a graph representation. Each pixel is treated as a node, and edges reflect similarity in color, intensity, or texture. By performing clustering in the low-dimensional space defined by the leading eigenvectors, spectral clustering can effectively separate complex or non-convex regions, making it particularly useful for segmenting images with irregular shapes or subtle boundaries.

\textbf{Graph-based formulation:}

\begin{enumerate}
    \item Construct an affinity matrix $W$ where $W_{ij}$ represents similarity between points $i$ and $j$:
    \begin{equation}
        W_{ij} = \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\right)
    \end{equation}
    
    \item Compute the graph Laplacian. The normalized Laplacian is:
    \begin{equation}
        L_{norm} = I - D^{-1/2}WD^{-1/2}
    \end{equation}
    where $D$ is the diagonal degree matrix with $D_{ii} = \sum_j W_{ij}$
    
    \item Compute the $k$ smallest eigenvectors of $L_{norm}$
    
    \item Form matrix $U \in \mathbb{R}^{n \times k}$ from these eigenvectors
    
    \item Apply K-means to the rows of $U$
\end{enumerate}

\begin{tipbox}
Spectral clustering is particularly effective for image segmentation because it can capture non-convex cluster shapes and naturally incorporates spatial relationships through the affinity matrix.
\end{tipbox}

\textbf{Normalized Cuts:} A popular spectral method for image segmentation that minimizes:

\begin{equation}
    \text{Ncut}(A, B) = \frac{\text{cut}(A, B)}{\text{assoc}(A, V)} + \frac{\text{cut}(A, B)}{\text{assoc}(B, V)}
\end{equation}

where $\text{cut}(A, B)$ is the total weight of edges between segments $A$ and $B$, and $\text{assoc}(A, V)$ is the total weight of edges from $A$ to all nodes.

\subsection{Density-Based Clustering (DBSCAN)}

DBSCAN (Density-Based Spatial Clustering of Applications with Noise) groups pixels into clusters based on local density, marking sparse regions as noise.

\begin{definitionbox}[DBSCAN Core Concepts]
DBSCAN defines clusters using two parameters:
\begin{itemize}
    \item $\varepsilon$ (eps): The neighborhood radius
    \item MinPts: Minimum number of points required to form a dense region
\end{itemize}
A point $p$ is a \textbf{core point} if at least MinPts points are within distance $\varepsilon$ of $p$.
\end{definitionbox}

\textbf{Algorithm steps:}
\begin{enumerate}
    \item Find core points based on $\varepsilon$ and MinPts
    \item Connect core points that are within $\varepsilon$ of each other
    \item Assign non-core points to nearby clusters or mark as noise
\end{enumerate}

\textbf{Advantages for image segmentation:}
\begin{itemize}
    \item Can find arbitrarily shaped clusters (unlike K-means)
    \item Does not require specifying number of clusters $k$ in advance
    \item Naturally handles outliers and noise
    \item Particularly useful for images with irregular region shapes
\end{itemize}

\begin{warningbox}
DBSCAN's results depend heavily on setting $\varepsilon$ (neighborhood radius) and MinPts correctly. Poor parameter choices can merge distinct regions or split coherent ones.
\end{warningbox}

\subsection{Other Classical Methods}

Several other clustering methods are used for image segmentation:

\begin{itemize}
    \item \textbf{Mean-Shift Clustering:} A non-parametric technique that iteratively shifts points toward modes (local maxima) of the density function. Does not require specifying $k$.
    
    \item \textbf{Gaussian Mixture Models (GMM):} A probabilistic approach that models data as a mixture of Gaussian distributions, providing soft cluster assignments similar to FCM.
    
    \item \textbf{Graph Cuts:} Energy minimization methods that formulate segmentation as finding optimal cuts in a graph, balancing data fidelity and smoothness terms.
    
    \item \textbf{Watershed Algorithm:} A morphological approach treating the image as a topographic surface and finding basin boundaries.
\end{itemize}

\begin{notebox}[Motivation for Quantum Approaches]
Classical clustering methods typically minimize simple distance-based cost functions (like within-cluster variance) and lack mechanisms to easily exploit very high-dimensional structure. This motivates exploring quantum techniques, which can embed data in quantum states and optimize quantum-native objectives that may better capture complex relationships in image data.
\end{notebox}

\section{Computational Bottlenecks: The Case for Quantum Advantage}

Despite their effectiveness, classical clustering algorithms face significant computational challenges when applied to modern image segmentation tasks. This section analyzes these bottlenecks and motivates the exploration of quantum computing solutions.

\subsection{Scalability Issues in Classical Methods}

Modern imaging systems produce increasingly large datasets. A single 4K image contains over 8 million pixels, each potentially represented by multiple features. The computational complexity of clustering algorithms presents serious scalability challenges:

\begin{table}[h]
\centering
\caption{Computational Complexity of Classical Clustering Algorithms}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Algorithm} & \textbf{Time Complexity} & \textbf{Space Complexity} \\
\midrule
K-Means & $O(nkdI)$ & $O(nd + kd)$ \\
Hierarchical & $O(n^3)$ & $O(n^2)$ \\
Fuzzy C-Means & $O(nc^2dI)$ & $O(nd + cd)$ \\
Spectral & $O(n^3)$ & $O(n^2)$ \\
\bottomrule
\end{tabular}
\label{tab:complexity}
\end{table}

Here, $n$ is the number of data points, $k$ or $c$ is the number of clusters, $d$ is dimensionality, and $I$ is the number of iterations.

\begin{warningbox}
For spectral clustering on a 1-megapixel image, computing the affinity matrix alone requires $10^{12}$ operations and $10^{12}$ bytes of storage—clearly impractical for real-time applications.
\end{warningbox}

\subsection{High-Dimensional Data Challenges}

Image segmentation often requires high-dimensional feature spaces to capture color, texture, and spatial information. This leads to the \textit{curse of dimensionality}:

\begin{itemize}
    \item \textbf{Distance concentration:} In high dimensions, the ratio of nearest to farthest neighbor distances approaches 1, making distance-based methods less discriminative
    \item \textbf{Sparse data:} Data becomes increasingly sparse as dimensions increase, requiring exponentially more samples
    \item \textbf{Computational overhead:} Distance calculations scale linearly with dimensionality
\end{itemize}

\begin{definitionbox}[Curse of Dimensionality]
For uniformly distributed data in a $d$-dimensional hypercube, the expected distance to the nearest neighbor grows as:
\begin{equation}
    E[d_{NN}] \propto \left(\frac{1}{n}\right)^{1/d}
\end{equation}
As $d \to \infty$, all points become approximately equidistant.
\end{definitionbox}

\subsection{Quantum Advantage: Foundations and Potential}

\textbf{Quantum advantage} (or quantum supremacy) refers to the demonstration that a quantum computer can solve a problem that is practically impossible for classical computers within a reasonable timeframe. To understand why quantum computing may benefit clustering, we must first examine the relevant computational complexity classes.

\begin{definitionbox}[Complexity Classes]
Key complexity classes relevant to quantum computing:
\begin{itemize}
    \item \textbf{P} (Polynomial time): Problems solvable efficiently on classical computers
    \item \textbf{NP} (Nondeterministic Polynomial time): Problems whose solutions can be verified efficiently
    \item \textbf{BQP} (Bounded-error Quantum Polynomial time): Problems solvable efficiently on quantum computers with bounded error probability
\end{itemize}
\end{definitionbox}

The relationship between these classes is believed to be:
\begin{equation}
    \text{P} \subset \text{BQP} \subseteq \text{NP}
\end{equation}

This hierarchy suggests that quantum computers can solve some problems more efficiently than classical computers. Clustering is generally \textbf{NP-hard} for arbitrary distance metrics, as optimal partitioning involves exploring an exponentially large solution space—making it a natural candidate for quantum speedups.

\begin{keyconceptbox}[Quantum Advantages for Clustering]
Quantum computing offers several fundamental advantages:
\begin{enumerate}
    \item \textbf{Exponential state space:} $n$ qubits can represent $2^n$ states simultaneously through superposition
    \item \textbf{Quantum parallelism:} Operations can be performed on all superposition states at once
    \item \textbf{Amplitude encoding:} $N$ classical data points can be encoded in $\log_2 N$ qubits
    \item \textbf{Algorithmic speedups:} Grover's search provides quadratic speedup; HHL algorithm offers exponential speedup for linear systems
\end{enumerate}
\end{keyconceptbox}

\textbf{Theoretical speedups for clustering-related tasks:}

\begin{itemize}
    \item \textbf{Distance calculation:} Quantum algorithms can compute distances between vectors in $O(\log d)$ time using amplitude encoding, compared to $O(d)$ classically
    \item \textbf{Eigenvalue problems:} Quantum phase estimation can find eigenvalues in $O(\text{poly}(\log n))$ time, potentially improving spectral clustering
    \item \textbf{Optimization:} Quantum approximate optimization algorithms (QAOA) and variational quantum eigensolvers (VQE) offer new approaches to NP-hard clustering problems
\end{itemize}

\begin{warningbox}[Current Limitations]
While quantum advantages are theoretically promising, practical implementation faces significant challenges:
\begin{itemize}
    \item Limited qubit counts on current hardware
    \item Noise and decoherence in the NISQ (Noisy Intermediate-Scale Quantum) era
    \item Overhead of quantum data encoding (the ``data loading problem'')
    \item Proving definitive quantum advantage for clustering remains an active research area
\end{itemize}
\end{warningbox}
